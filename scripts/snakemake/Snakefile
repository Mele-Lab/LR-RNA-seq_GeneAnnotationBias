import pandas as pd

p = os.getcwd()
sys.path.append(p)

from utils import *

include: 'blast.smk'
include: 'samtools.smk'
include: 'formatting.smk'
include: 'minimap.smk'
include: 'splitting.smk'

configfile: '/gpfs/projects/bsc83/Projects/pantranscriptome/pclavell/ONT_preprocessing/scripts/snakemake/config.yml'

df = pd.read_csv('/gpfs/projects/bsc83/Projects/pantranscriptome/pclavell/ONT_preprocessing/scripts/snakemake/test_data_2.tsv', sep='\t')

def get_df_val(df, col, sample):
    temp = df.loc[df['sample']==sample]
    assert len(temp.index) == 1
    return temp[col].values[0]


rule all:
    input:
        expand(config['data']['split']['mockout3'],
                sample=df['sample'].tolist())#,
#        expand(config['data']['minimap']['bam_ind']
#               sample=df['sample'].tolist())


# index unaligned bam so next rule can work properly
use rule bamindex as index_unalignedbam with:
    input:
        bam = lambda wc: get_df_val(df, 'fname', wc.sample)
    output:
        index = config['data']['duplex']['index']

# get duplex status of each read and record in new bam
# corresponds to scripts/00.1_append_samtag.sh
use rule add_duplex_status as basecall_add_duplex_status with:
    input:
        unbam = lambda wc: get_df_val(df, 'fname', wc.sample),
        index = rules.index_unalignedbam.output.index
    output:
        unbam = config['data']['duplex']['bam']


# BAM to fastQ
use rule bam_to_fastq as unalignedbam_to_fastq with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    output:
        fastq = config['data']['duplex']['fastq']



# Step 1 splitting reads with both ONT adapter and full CapTrap linkers
use rule split_ONT_plus_full_linker as first_split with:
    input:
        rules.unalignedbam_to_fastq.output.fastq
    params:
        fastqdir = config['data']['split']['fastqdir1'],
        outdir = config['data']['split']['outdir1']
    output:
        mockout = config['data']['split']['mockout1']
        # out2 = config['data']['split']['outsplit1_2'],
        # out3 = config['data']['split']['outsplit1_3']

# Step 2 splitting reads with full CapTrap linkers
use rule split_full_linker as second_split with:
    input:
        mockin2 = rules.first_split.output.mockout
    params:
        fastqdir = config['data']['split']['mockout1'],
        outdir = config['data']['split']['outdir2']
    output:
        mockout = config['data']['split']['mockout2']
        #out1 = config['data']['split']['outsplit2_1'],
        #out2 = config['data']['split']['outsplit2_2'],
        #out3 = config['data']['split']['outsplit2_3']

# Step 3 remove multisplits
use rule skip_multisplits as remove_multisplits with:
    input:
        mockinput = rules.second_split.output.mockout
    params:
        splitdir = config['data']['split'],
        sample = config['sample'],
        outdir = config['data']['split']['multiout']
    output:
        mockout = config['data']['split']['mockout3']
        #fastq_split = config['data']['split']['fastq_split']
        # fastq_multisplit = config['data']['split']['fastq_multisplit'],
        # multisplitsID1 = config['data']['split']['multisplitsID1'],
        # multisplitsID2 = config['data']['split']['multisplitsID2']



# START OF DEDUPLICATION
# corr. to 00_fastq2fasta.sh
use rule fq_to_fa as raw_fq_to_fa with:
    input:
        fq = config['data']['split']['fastq_split']
        # fq = lambda wc: get_df_val(df, 'fname', wc.sample)
        # fq = config['raw']['fq']
    output:
        fa = config['data']['duplex']['fa']

# corr. to 01_blast/00_create_db.sh
use rule make_blast_db as blast_db with:
    input:
        fa = config['ref']['linker_5_fa']
    params:
        db = config['data']['blast']['db']
    output:
        nhr = config['data']['blast']['nhr'],
        nin = config['data']['blast']['nin'],
        nsq = config['data']['blast']['nsq']

# corr to 01_blast/01_blast.sh
use rule blast as blast_seq with:
    input:
        fa = rules.raw_fq_to_fa.output.fa,
        nhr = rules.blast_db.output.nhr
    params:
        db = rules.blast_db.params.db
    output:
        xml = config['data']['blast']['xml']

# corr to 01_blast/02_xml2sam
use rule blast2bam as blast2bam_seq with:
    input:
        ref_fa = config['ref']['fa'],
        fa = rules.blast_seq.input.fa,
        xml = rules.blast_seq.output.xml
    params:
        blast2bam = config['bin']['blast2bam']
    output:
        sam = temporary(config['data']['blast']['sam'])

use rule sam_filt_unmapped as filt_sam_blast with:
    input:
        align = rules.blast2bam_seq.output.sam
    output:
        align = config['data']['blast']['sam_filt']
        # align = temporary(config['data']['blast']['sam_filt'])

use rule sam_to_bam as sam_to_bam_blast with:
    input:
        align = rules.filt_sam_blast.output.align
    output:
        align = temporary(config['data']['blast']['bam'])

use rule sam_sort as bam_sort_blast with:
    input:
        align = rules.sam_to_bam_blast.output.align
    output:
        align = config['data']['blast']['bam_sort']

use rule bam_index as bam_index_blast with:
    input:
        align = rules.bam_sort_blast.output.align
    output:
        align = config['data']['blast']['bam_ind']

# corr. to 02_extract_UMI/01_extract_UMI.sh
use rule extract_umi as extract_umi_blast with:
    input:
        align = rules.filt_sam_blast.output.align
    params:
        opref = config['data']['umi']['umis'].split('_extracted_UMI.tsv')[0]
    output:
        umis = config['data']['umi']['umis'],
        fa = config['data']['umi']['fa']

# TODO this is where the duplex read recovery will happen

# corr. to 03_genome_mapping/01_minimap.sh
use rule minimap as map with:
    input:
        junc_bed = config['ref']['junc_bed'],
        fa = rules.extract_umi_blast.output.fa,
        ref_fa = config['ref']['fa']
    output:
        sam = config['data']['minimap']['sam']
        # sam = temporary(config['data']['minimap']['sam'])

# rules below corr. to 03_genome_mapping/02_filter_and_transform_results
use rule align_filt_unmapped_supp as filt_sam_map with:
    input:
        align = rules.map.output.sam
    output:
        align = config['data']['minimap']['sam_filt']

use rule sam_to_bam as sam_to_bam_map with:
    input:
        align = rules.filt_sam_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam'])

use rule sam_sort as bam_sort_map with:
    input:
        align = rules.sam_to_bam_map.output.align
    output:
        align = config['data']['minimap']['bam_sort']

use rule bam_index as bam_index_map with:
    input:
        align = rules.bam_sort_map.output.align
    output:
        align = config['data']['minimap']['bam_ind']
