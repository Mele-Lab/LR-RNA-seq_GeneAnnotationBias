import pandas as pd

p = os.getcwd()
sys.path.append(p)

from utils import *

include: 'blast.smk'
include: 'samtools.smk'
include: 'formatting.smk'
include: 'minimap.smk'
include: 'splitting.smk'

configfile: 'snakemake/config.yml'
# df = pd.read_csv('test_data.tsv', sep='\t')
df = pd.read_csv('snakemake/test_data_2.tsv', sep='\t')

wildcard_constraints:
    sample='|'.join([re.escape(x) for x in df['sample'].tolist()]),

def get_df_val(df, col, sample):
    temp = df.loc[df['sample']==sample]
    assert len(temp.index) == 1
    return temp[col].values[0]


rule all:
    input:
        expand(config['data']['minimap']['bam_ind'],
                sample=df['sample'].tolist())#,
#        expand(config['data']['minimap']['bam_ind']
#               sample=df['sample'].tolist())


# index unaligned bam so next rule can work properly
# use rule bamindex as index_unalignedbam with:
#     input:
#         bam = lambda wc: get_df_val(df, 'fname', wc.sample)
#     output:
#         index = config['data']['duplex']['index']

# get duplex status of each read and record in new bam
# corresponds to scripts/00.1_append_samtag.sh
use rule add_duplex_status as basecall_add_duplex_status with:
    input:
        unbam = lambda wc: get_df_val(df, 'fname', wc.sample),
        # index = rules.index_unalignedbam.output.index
    output:
        unbam = config['data']['duplex']['bam']


# BAM to fastQ
use rule bam_to_fastq as unalignedbam_to_fastq with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    output:
        fastq = config['data']['duplex']['fq']



# Step 1 splitting reads with both ONT adapter and full CapTrap linkers
use rule split_ONT_plus_full_linker as first_split with:
    input:
        rules.unalignedbam_to_fastq.output.fastq
    params:
        fastqdir = config['data']['split']['fastqdir1'],
        outdir = config['data']['split']['outdir1']
    output:
        mockout = config['data']['split']['mockout1']
        # out2 = config['data']['split']['outsplit1_2'],
        # out3 = config['data']['split']['outsplit1_3']

# Step 2 splitting reads with full CapTrap linkers
use rule split_full_linker as second_split with:
    input:
        mockin2 = rules.first_split.output.mockout
    params:
        fastqdir = config['data']['split']['outdir1'],
        outdir = config['data']['split']['outdir2']
    output:
        mockout = config['data']['split']['mockout2']
        #out1 = config['data']['split']['outsplit2_1'],
        #out2 = config['data']['split']['outsplit2_2'],
        #out3 = config['data']['split']['outsplit2_3']

# get the read IDs of all reads
use rule sam_get_read_ids as get_all_read_ids with:
    input:
        align = rules.basecall_add_duplex_status.output.align
    output:
        txt = config['data']['duplex']['read_ids']

# Step 3 remove multisplits
use rule skip_multisplits as remove_multisplits with:
    input:
        mockinput = rules.second_split.output.mockout
    params:
        splitdir = config['data']['split']['dirroot'],
        outdir = config['data']['split']['multiout']
    output:
        mockout = config['data']['split']['mockout3']
        #fastq_split = config['data']['split']['fastq_split']
        # fastq_multisplit = config['data']['split']['fastq_multisplit'],
        # multisplitsID1 = config['data']['split']['multisplitsID1'],
        # multisplitsID2 = config['data']['split']['multisplitsID2']



# START OF DEDUPLICATION
# corr. to 00_fastq2fasta.sh
use rule fq_to_fa as raw_fq_to_fa with:
    input:
        fq = config['data']['split']['fastq_split']
        # fq = lambda wc: get_df_val(df, 'fname', wc.sample)
        # fq = config['raw']['fq']
    output:
        fa = config['data']['duplex']['fa']

# corr. to 01_blast/00_create_db.sh
use rule make_blast_db as blast_db with:
    input:
        fa = config['ref']['linker_5_fa']
    params:
        db = config['data']['blast']['db']
    output:
        nhr = config['data']['blast']['nhr'],
        nin = config['data']['blast']['nin'],
        nsq = config['data']['blast']['nsq']

# corr to 01_blast/01_blast.sh
use rule blast as blast_seq with:
    input:
        fa = rules.raw_fq_to_fa.output.fa,
        nhr = rules.blast_db.output.nhr
    params:
        db = rules.blast_db.params.db
    output:
        xml = config['data']['blast']['xml']

# corr to 01_blast/02_xml2sam
use rule blast2bam as blast2bam_seq with:
    input:
        ref_fa = config['ref']['linker_5_fa'],
        fa = rules.blast_seq.input.fa,
        xml = rules.blast_seq.output.xml
    params:
        blast2bam = config['bin']['blast2bam']
    output:
        sam = config['data']['blast']['sam']

# filter out unmapped reads
use rule sam_filt_unmapped as filt_sam_blast with:
    input:
        align = rules.blast2bam_seq.output.sam
    output:
        align = config['data']['blast']['sam_filt']
        # align = temporary(config['data']['blast']['sam_filt'])

# filter out supplementary and secondary reads (ie only primary)
use rule sam_filt_for_primary as filt_sam_blast_2 with:
    input:
        align = rules.filt_sam_blast.output.align
    output:
        align = config['data']['blast']['sam_filt_2']

use rule sam_to_bam as sam_to_bam_blast with:
    input:
        align = rules.filt_sam_blast_2.output.align
    output:
        align = temporary(config['data']['blast']['bam'])

use rule sam_sort as bam_sort_blast with:
    input:
        align = rules.sam_to_bam_blast.output.align
    output:
        align = config['data']['blast']['bam_sort']

use rule bam_index as bam_index_blast with:
    input:
        align = rules.bam_sort_blast.output.align
    output:
        align = config['data']['blast']['bam_ind']

# corr. to 02_extract_UMI/01_extract_UMI.sh
use rule extract_umi as extract_umi_blast with:
    input:
        align = rules.filt_sam_blast.output.align
    params:
        opref = config['data']['umi']['umis'].split('_extracted_UMI.tsv')[0],
        sep = config['params']['umi_sep']
    output:
        umis = config['data']['umi']['umis'],
        fa = config['data']['umi']['fa']

use rule fasta_get_read_ids as get_umi_read_ids with:
    input:
        fa = rules.extract_umi_blast.output.fa
    output:
        txt = config['data']['umi']['read_ids']

# get the read ids of reads that we didn't find UMIs for
# by subtracting the reads with a UMI from the list of all reads
use rule read_id_diff as get_non_umi_read_ids with:
    input:
        a = rules.get_all_read_ids.output.txt,
        b = rules.get_umi_read_ids.output.txt
    output:
        txt = config['data']['umi']['no_umi_read_ids']

# TODO this is where the duplex read recovery will happen

# corr. to 03_genome_mapping/01_minimap.sh
use rule minimap as map with:
    input:
        junc_bed = config['ref']['junc_bed'],
        fa = rules.extract_umi_blast.output.fa,
        ref_fa = config['ref']['fa']
    output:
        sam = config['data']['minimap']['sam']
        # sam = temporary(config['data']['minimap']['sam'])

# rules below corr. to 03_genome_mapping/02_filter_and_transform_results
use rule align_filt_unmapped_supp as filt_sam_map with:
    input:
        align = rules.map.output.sam
    output:
        align = config['data']['minimap']['sam_filt']

use rule sam_to_bam as sam_to_bam_map with:
    input:
        align = rules.filt_sam_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam'])

use rule sam_sort as bam_sort_map with:
    input:
        align = rules.sam_to_bam_map.output.align
    output:
        align = config['data']['minimap']['bam_sort']

use rule bam_index as bam_index_map with:
    input:
        align = rules.bam_sort_map.output.align
    output:
        align = config['data']['minimap']['bam_ind']

# rules below correspond to 05_deduplication/01_deduplication.sh
use rule gtf_to_gt_map as get_gt_map with:
    input:
        gtf = config['ref']['gtf']
    output:
        gt_map = config['ref']['gt_map']

use rule dedupe_umi as umi_dedupe_reads with:
    input:
        align = rules.bam_sort_map.output.align,
        gt_map = config['ref']['gt_map']
    params:
        sep = config['params']['umi_sep'],
        edit_dist = 2
    output:
        align = config['data']['umi_dedupe']['bam'],
        log = config['data']['umi_dedupe']['log']

# get read IDs of reads that we UMI deduplicate
use rule sam_get_read_ids as get_umi_dedupe_read_ids with:
    input:
        align = rules.umi_dedupe_reads.output.align
    output:
        txt = config['data']['umi_dedupe']['read_ids']

# get bam file of
# 1. UMI deduplicated reads
# 2. reads that we didn't find UMIs for
use rule read_id_union as get_qc_reads with:
    input:
        a = rules.get_non_umi_read_ids.output.txt,
        b = rules.get_umi_dedupe_read_ids.output.txt
    output:
        txt = config['data']['qc']['read_ids']

use rule sam_filt_for_read_ids as make_qc_bam with:
    input:
        align = rules.get_all_read_ids.input.align,
        read_ids = rules.get_qc_reads.output.txt
    output:
        align = config['data']['qc']['bam']
