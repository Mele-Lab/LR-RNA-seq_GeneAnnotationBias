import pandas as pd

p = os.getcwd()
sys.path.append(p)

from utils import *

include: 'smkmod/blast.smk'
include: 'smkmod/samtools.smk'
include: 'smkmod/formatting.smk'
include: 'smkmod/minimap.smk'
include: 'smkmod/splitting.smk'
include: 'smkmod/qc.smk'
include: 'smkmod/clean.smk'

configfile: 'snakemake/config.yml'
#config_tsv = 'snakemake/test_data_2.tsv'
config_tsv = '/gpfs/projects/bsc83/Projects/pantranscriptome/pclavell/ONT_preprocessing/test_data_2.tsv'



df = pd.read_csv(config_tsv, sep='\t')

def get_df_val(df, col, sample):
    temp = df.loc[df['sample']==sample]
    assert len(temp.index) == 1
    return temp[col].values[0]

# HERE WE CHOOSE THE DESIRED FINAL OUTPUTS
rule all:
    input:
        expand(config['data']['output']['q7fastqgz'],
                sample=df['sample'].tolist()),
        expand(config['data']['qc']['readnum'],
                sample=df['sample'].tolist()),
        expand(config['data']['qc']['nanostats'],
                sample=df['sample'].tolist()),
        expand(config['data']['qc']['plotpdf'],
                sample=df['sample'].tolist()),
        expand(config['data']['qc']['fastqcreport'],
                sample=df['sample'].tolist()),
        expand(config['clean']['clean_split3'],
                sample=df['sample'].tolist())


# index unaligned bam so next rule can work properly
use rule bamindex as index_unalignedbam with:
    input:
        bam = lambda wc: get_df_val(df, 'fname', wc.sample)
    output:
        index = temporary(config['data']['duplex']['index'])

# get duplex status of each read and record in new bam
# corresponds to scripts/00.1_append_samtag.sh
use rule add_duplex_status as basecall_add_duplex_status with:
    input:
        unbam = lambda wc: get_df_val(df, 'fname', wc.sample),
        index = rules.index_unalignedbam.output.index
    params:
        sep= ":"
    output:
        unbam = temporary(config['data']['duplex']['bam'])


# BAM to fastQ
use rule bam_to_fastq as unalignedbam_to_fastq with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    output:
        fastqgz = temporary(config['data']['duplex']['fqgz'])


# Step 1 splitting reads with both ONT adapter and full CapTrap linkers
use rule split_ONT_plus_full_linker as first_split with:
    input:
        rules.unalignedbam_to_fastq.output.fastqgz
    params:
        fastqdir = config['data']['split']['fastqdir1'],
        outdir = config['data']['split']['outdir1']
    output:
        mockout = temporary(config['data']['split']['mockout1'])


# Step 2 splitting reads with full CapTrap linkers
use rule split_full_linker as second_split with:
    input:
        mockin2 = rules.first_split.output.mockout
    params:
        fastqdir = config['data']['split']['outdir1'],
        outdir = config['data']['split']['outdir2']
    output:
        mockout = temporary(config['data']['split']['mockout2'])


# Step 3 remove multisplits
use rule skip_multisplits as remove_multisplits with:
    input:
        mockinput = rules.second_split.output.mockout
    params:
        splitdir = config['data']['split']['dirroot'],
        outdir = config['data']['split']['multiout']
    output:
        fq = config['data']['split']['fastq_split']

# get the read IDs of all reads
use rule fastqgz_get_read_ids as get_all_read_ids with:
    input:
        fq = rules.remove_multisplits.output.fq
    output:
        txt = temporary(config['data']['duplex']['read_ids'])

############ START OF DEDUPLICATION ##################################

# convert to fasta so later we can blast
# corr. to 00_fastq2fasta.sh
use rule fq_to_fa as raw_fq_to_fa with:
    input:
        fq = config['data']['split']['fastq_split']
    output:
        fa = temporary(config['data']['duplex']['fa'])


# create blast database with 5'linker sequence
# corr. to 01_blast/00_create_db.sh
use rule make_blast_db as blast_db with:
    input:
        fa = config['ref']['linker_5_fa']
    params:
        db = config['data']['blast']['db']
    output:
        nhr = config['data']['blast']['nhr'],
        nin = config['data']['blast']['nin'],
        nsq = config['data']['blast']['nsq']

# blast our reads to the 5'linker database
# corr to 01_blast/01_blast.sh
use rule blast as blast_seq with:
    input:
        fa = rules.raw_fq_to_fa.output.fa,
        nhr = rules.blast_db.output.nhr
    params:
        db = rules.blast_db.params.db
    output:
        xml = temporary(config['data']['blast']['xml'])

# as the previous step generates an xml (so it can be converted to sam that is not corrupt)
#   we convert the xml to sam
# corr to 01_blast/02_xml2sam
use rule blast2bam as blast2bam_seq with:
    input:
        ref_fa = config['ref']['linker_5_fa'],
        fa = rules.blast_seq.input.fa,
        xml = rules.blast_seq.output.xml
    params:
        blast2bam = config['bin']['blast2bam']
    output:
        sam = temporary(config['data']['blast']['sam'])

# filter out unmapped reads
use rule sam_filt_unmapped as filt_sam_blast with:
    input:
        align = rules.blast2bam_seq.output.sam
    output:
        align = temporary(config['data']['blast']['sam_filt'])

# filter out supplementary and secondary reads (ie only primary)
use rule sam_filt_for_primary as filt_sam_blast_2 with:
    input:
        align = rules.filt_sam_blast.output.align
    output:
        align = temporary(config['data']['blast']['sam_filt_2'])

# convert sam to bam
use rule sam_to_bam as sam_to_bam_blast with:
    input:
        align = rules.filt_sam_blast_2.output.align
    output:
        align = temporary(config['data']['blast']['bam'])

# sort the bam
use rule bam_sort as bam_sort_blast with:
    input:
        align = rules.sam_to_bam_blast.output.align
    output:
        align = temporary(config['data']['blast']['bam_sort'])

# index the bam
use rule bam_index as bam_index_blast with:
    input:
        align = rules.bam_sort_blast.output.align
    output:
        align = temporary(config['data']['blast']['bam_ind'])

# extract the UMIs
# corr. to 02_extract_UMI/01_extract_UMI.sh
use rule extract_umi as extract_umi_blast with:
    input:
        align = rules.filt_sam_blast.output.align,
        index = rules.bam_index_blast.output.align
    params:
        opref = config['data']['umi']['umis'].split('_extracted_UMI.tsv')[0],
        sep = config['params']['umi_sep']
    output:
        umis = temporary(config['data']['umi']['umis']),
        fa = temporary(config['data']['umi']['fa'])

# get the readID of reads with UMI
use rule fasta_get_read_ids_rm_umi as get_umi_read_ids with:
    input:
        fa = rules.extract_umi_blast.output.fa
    output:
        txt = temporary(config['data']['umi']['read_ids'])

# get the read ids of reads that we didn't find UMIs for
#   by subtracting the reads with a UMI from the list of all reads
use rule read_id_diff as get_non_umi_read_ids with:
    input:
        a = rules.get_all_read_ids.output.txt,
        b = rules.get_umi_read_ids.output.txt
    output:
        txt = temporary(config['data']['umi']['no_umi_read_ids'])


# create an index for minimap mapping
rule minimap_index:
    resources:
        threads = 112
    params:
        ref = config['ref']['fa']
    output:
        index = config['ref']['mmi']
    shell:
        """
        module load minimap2

        minimap2 -t {resources.threads} -d {output.index} {params.ref}

        """

# map reads with UMI to the transcriptome whose transcriptID have been simplified to include only ENSEMBLcode.version
# corr. to 03_genome_mapping/01_minimap.sh
use rule minimap as map with:
    input:
        junc_bed = config['ref']['junc_bed'],
        fa = rules.extract_umi_blast.output.fa,
        ref_fa = config['ref']['mmi']
    output:
        sam = temporary(config['data']['minimap']['sam'])

# filter unmapped and supplementary alignemnts
# rules below corr. to 03_genome_mapping/02_filter_and_transform_results
use rule align_filt_unmapped_supp as filt_sam_map with:
    input:
        align = rules.map.output.sam
    output:
        align = temporary(config['data']['minimap']['sam_filt'])

# convert sam to bam
use rule sam_to_bam as sam_to_bam_map with:
    input:
        align = rules.filt_sam_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam'])

# sort the bam
use rule bam_sort as bam_sort_map with:
    input:
        align = rules.sam_to_bam_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam_sort'])

# index the bam
use rule bam_index as bam_index_map with:
    input:
        align = rules.bam_sort_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam_ind'])

# get read ID of reads with UMI but unmapped

use rule bam_get_unmapped_read_ids as get_bam_get_unmapped_read_ids with:
    input:
        bam = rules.map.output.sam
    output:
        txt = temporary(config['data']['minimap']['read_ids'])

# convert gtf to a gene to transcripts correspondence map
# rules below correspond to 05_deduplication/01_deduplication.sh
use rule gtf_to_gt_map as get_gt_map with:
    input:
        gtf = config['ref']['gtf']
    output:
        gt_map = config['ref']['gt_map']

# umi deduplication of reads aligned to the same gene and with concordant UMI
use rule dedupe_umi as umi_dedupe_reads with:
    input:
        align = rules.bam_sort_map.output.align,
        mock = rules.bam_index_map.output.align,
        gt_map = config['ref']['gt_map']
    params:
        sep = config['params']['umi_sep'],
        edit_dist = 2
    output:
        align = temporary(config['data']['umi_dedupe']['bam']),
        log = config['data']['umi_dedupe']['log']


# get read IDs of reads that we UMI deduplicate
use rule bam_get_read_ids as get_umi_dedupe_read_ids with:
    input:
        bam = rules.umi_dedupe_reads.output.align
    output:
        txt = temporary(config['data']['umi_dedupe']['read_ids'])




# identify reads to create a new fastq containing:
# 1. UMI deduplicated reads
# 2. reads that we didn't find UMIs for or were unmapped to transcriptome
use rule read_id_union as get_qc_reads with:
    input:
        a = rules.get_non_umi_read_ids.output.txt,
        b = rules.get_umi_dedupe_read_ids.output.txt,
        c = rules.get_bam_get_unmapped_read_ids.output.txt
    params:
        temporalfile = config['data']['qc']['temporalfile'],
        text = config['data']['count']['parent_simplex']
    output:
        txt = temporary(config['data']['qc']['read_ids']),
        count_parent_simplex = temporary(config['data']['qc']['count_parent_simplex'])

#create a new fastq containing:
# 1. UMI deduplicated reads
# 2. reads that we didn't find UMIs for or were unmapped to transcriptome

use rule fastqgz_filter as make_clean_fastq with:
    input:
        fq = rules.remove_multisplits.output.fq,
        read_ids = rules.get_qc_reads.output.txt
    output:
        cleanfastq = temporary(config['data']['qc']['fq']),
        cleanfastqgz = config['data']['qc']['fqgz']


# Trim adapters
use rule porechop as trimadapters with:
    input:
        fastqgz = rules.make_clean_fastq.output.cleanfastqgz
    output:
        fastqgz = temporary(config['data']['output']['trimmed'])

# Filter based on phred Q 7 =20% error rate
use rule fastqfilter as filterfastq with:
    input:
        align = rules.trimadapters.output.fastqgz
    params:
        phred = 7
    output:
        align = config['data']['output']['q7fastqgz']

# obtaing length and average quality of reads
use rule pseudonanoplot as nanostats with:
    input:
        fastq = rules.filterfastq.output.align
    output:
        tsv = temporary(config['data']['qc']['nanostats'])


# Count final: concatenate all files
rule cat_all_counts:
    resources:
        threads = 1
    input:
        a = config['data']['qc']['readnum1'],
        b = config['data']['qc']['readnum2'],
        c = config['data']['qc']['readnum3'],
        d = config['data']['qc']['readnum4'],
        e = config['data']['qc']['readnum5'],
        f = config['data']['qc']['readnum6'],
        g = config['data']['qc']['readnum7'],
        h = config['data']['qc']['readnum8'],
        i = config['data']['qc']['readnum9'],
        j = config['data']['qc']['readnum10'],
        z = config['data']['qc']['count_parent_simplex'],
        k = config['data']['qc']['readnum10_2'],
        l = config['data']['qc']['readnum11'],
        m = config['data']['qc']['readnum11_2'],
        n = config['data']['qc']['readnum12']
    output:
        txt = config['data']['qc']['readnum']
    shell:
        """
        cat {input.a} {input.b} {input.c} {input.d} {input.e} {input.f} {input.g} {input.h} {input.i}\
        {input.j} {input.z} {input.k} {input.l} {input.m} {input.n}> {output.txt}
        """

# create custom QC report with info like length and Q distributions
use rule qcreport as create_qcreport with:
    input:
        nanooutput = rules.nanostats.output.tsv,
        readcount = rules.cat_all_counts.output.txt
    params:
        path = config['params']['path']
    output:
        pdf = config['data']['qc']['plotpdf'],
        globalstats = config['data']['qc']['reporttsv'],
        reads = config['data']['qc']['readspercentage'],
        reads2 = config['data']['qc']['readspercentage2']

# create fastqc report
use rule fastqc as fastqcreport with:
    input:
        fastq = rules.filterfastq.output.align
    params:
        adapters = config['ref']['adapters'],
        outdir = config['data']['qc']['fastqcreportdir']
    output:
        html = config['data']['qc']['fastqcreport']


########## COUNTING #################################################

# Count 1: how many reads do we start with
use rule count_unbam_reads as count_unbam_duplex_tag with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    params:
        text = config['data']['count']['unbam_duplex_tag']
    output:
        txt = temporary(config['data']['qc']['readnum1'])
# Count 2: reads after bam2fastq
use rule count_fq_gz_reads_from_params as count_bam2fastq with:
    input:
        mockin = config['data']['duplex']['fqgz']
    params:
        fqgz = rules.unalignedbam_to_fastq.output.fastqgz,
        text = config['data']['count']['fastq_from_bam']
    output:
        txt = temporary(config['data']['qc']['readnum2'])
# Count 3: reads after split1
use rule count_fq_gz_reads_from_params as count_fastgz_split1 with:
    input:
        mock = rules.first_split.output.mockout
    params:
        fqgz = config['data']['split']['fastqgz1'],
        text = config['data']['count']['fastqgz_split1']
    output:
        txt = temporary(config['data']['qc']['readnum3'])
# Count 4: reads after split2
use rule count_fq_gz_reads_from_params as count_fastgz_split2 with:
    input:
        mock = rules.second_split.output.mockout
    params:
        fqgz = config['data']['split']['fastqgz2'],
        text = config['data']['count']['fastqgz_split2']
    output:
        txt = temporary(config['data']['qc']['readnum4'])
# Count 5: reads after removemultisplit
use rule count_fq_gz_reads_from_params as count_fastgz_split3 with:
    input:
        fqgz = rules.remove_multisplits.output.fq
    params:
        fqgz = rules.remove_multisplits.output.fq,
        text = config['data']['count']['fastqgz_split3']
    output:
        txt = temporary(config['data']['qc']['readnum5'])
# Count 6: reads after fastq to fasta
use rule count_fa_reads as count_fasta_reads with:
    input:
        fa = rules.raw_fq_to_fa.output.fa
    params:
        text = config['data']['count']['fastafromfastq']
    output:
        txt = temporary(config['data']['qc']['readnum6'])
# Count 7: number of alignments to 5linker
use rule count_sam_mappings as count_total_5linker_mappings with:
    input:
        sam = rules.blast2bam_seq.output.sam
    params:
        text = config['data']['count']['5linkermappings']
    output:
        txt = temporary(config['data']['qc']['readnum7'])

# Count 8: number of primary alignments
use rule count_reads_with_primary_alignment as count_5linker_primary_alignments with:
    input:
        sam = rules.blast2bam_seq.output.sam
    params:
        text = config['data']['count']['5linkerprimarymappings']
    output:
        txt = temporary(config['data']['qc']['readnum8'])

# Count 9: number of reads with UMI
use rule count_text_reads as count_reads_with_UMI with:
    input:
        txt = rules.get_umi_read_ids.output.txt
    params:
        text = config['data']['count']['reads_with_UMI']
    output:
        txt = temporary(config['data']['qc']['readnum9'])
# Count 10: number of reads without UMI
use rule count_text_reads as count_reads_without_UMI with:
    input:
        txt = rules.get_non_umi_read_ids.output.txt
    params:
        text = config['data']['count']['reads_without_UMI']
    output:
        txt = temporary(config['data']['qc']['readnum10'])
# Count 10_2 reads with UMI but unmapped

use rule count_text_reads as count_reads_withUMI_non_deduped with:
    input:
        txt = rules.get_bam_get_unmapped_read_ids.output.txt
    params:
        text = config['data']['count']['reads_umi_non_deduped']
    output:
        txt = temporary(config['data']['qc']['readnum10_2'])
# Count 11: number of reads that could be potentially deduplicated
use rule count_text_reads as count_reads_potentially_dedup with:
    input:
        txt = rules.get_umi_dedupe_read_ids.output.txt
    params:
        text = config['data']['count']['reads_deduped']
    output:
        txt = temporary(config['data']['qc']['readnum11'])

# Count 11.2
use rule count_text_reads as get_reads_to_be_selected with:
    input:
        txt = rules.get_qc_reads.output.txt
    params:
        text = config['data']['count']['final_filtering']
    output:
        txt = temporary(config['data']['qc']['readnum11_2'])
# Count 12: number of reads after Q filter
use rule count_fq_gz_reads_from_params as count_final_reads with:
    input:
        mockin = config['data']['qc']['fqgz']
    params:
        fqgz = rules.filterfastq.output.align,
        text = config['data']['count']['final_count']
    output:
        txt = temporary(config['data']['qc']['readnum12'])


########## CLEANING STEPS ##########################################
use rule cleandir as clean_split1 with:
    input:
        mock = rules.count_fastgz_split1.output.txt
    params:
        inputdir = config['data']['split']['outdir1']
    output:
        cleanmock = config['clean']['clean_split1']

use rule cleandir as clean_split2 with:
    input:
        mock = rules.count_fastgz_split2.output.txt,
        mock2 = rules.clean_split1.output.cleanmock
    params:
        inputdir = config['data']['split']['outdir2']
    output:
        cleanmock = config['clean']['clean_split2']

use rule cleandir as clean_removemultisplits with:
    input:
        mock = rules.count_fastgz_split3.output.txt,
        mock2 = rules.clean_split2.output.cleanmock
    params:
        inputdir = config['data']['split']['multiout']
    output:
        cleanmock = config['clean']['clean_split3']