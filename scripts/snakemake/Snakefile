import pandas as pd

p = os.getcwd()
sys.path.append(p)

from utils import *

include: 'blast.smk'
include: 'samtools.smk'
include: 'formatting.smk'
include: 'minimap.smk'
include: 'splitting.smk'
include: 'qc.smk'

configfile: 'snakemake/config.yml'
# df = pd.read_csv('test_data.tsv', sep='\t')
#df = pd.read_csv('snakemake/test_data_2.tsv', sep='\t')


# wildcard_constraints:
#     sample='|'.join([re.escape(x) for x in df['sample'].tolist()]),

df = pd.read_csv('/gpfs/projects/bsc83/Projects/pantranscriptome/pclavell/ONT_preprocessing/scripts/snakemake/test_data_2.tsv', sep=' ')

def get_df_val(df, col, sample):
    temp = df.loc[df['sample']==sample]
    assert len(temp.index) == 1
    return temp[col].values[0]


rule all:
    input:
        expand(config['data']['qc']['fq'],
                sample=df['sample'].tolist()),
        expand(config['data']['qc']['readnum'],
                sample=df['sample'].tolist())


# index unaligned bam so next rule can work properly
use rule bamindex as index_unalignedbam with:
    input:
        bam = lambda wc: get_df_val(df, 'fname', wc.sample)
    output:
        index = temporary(config['data']['duplex']['index'])

# get duplex status of each read and record in new bam
# corresponds to scripts/00.1_append_samtag.sh
use rule add_duplex_status as basecall_add_duplex_status with:
    input:
        unbam = lambda wc: get_df_val(df, 'fname', wc.sample),
        index = rules.index_unalignedbam.output.index
    params:
        sep= ":"
    output:
        unbam = temporary(config['data']['duplex']['bam'])

# Count 1: how many reads do we start with
use rule count_unbam_reads as count_unbam_duplex_tag with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    params:
        text = config['data']['count']['unbam_duplex_tag']
    output:
        txt = temporary(config['data']['qc']['readnum1'])


# BAM to fastQ
use rule bam_to_fastq as unalignedbam_to_fastq with:
    input:
        unbam = rules.basecall_add_duplex_status.output.unbam
    output:
        fastqgz = temporary(config['data']['duplex']['fqgz'])

# Count 2: reads after bam2fastq
use rule count_fq_gz_reads as count_bam2fastq with:
    input:
        mockin = config['data']['duplex']['fqgz']
    params:
        fqgz = rules.unalignedbam_to_fastq.output.fastqgz,
        text = config['data']['count']['fastq_from_bam']
    output:
        txt = temporary(config['data']['qc']['readnum2'])


# Step 1 splitting reads with both ONT adapter and full CapTrap linkers
use rule split_ONT_plus_full_linker as first_split with:
    input:
        rules.unalignedbam_to_fastq.output.fastqgz
    params:
        fastqdir = config['data']['split']['fastqdir1'],
        outdir = config['data']['split']['outdir1']
    output:
        mockout = temporary(config['data']['split']['mockout1'])#,
        #fqgz = config['data']['split']['fastqgz1']
        # out2 = config['data']['split']['outsplit1_2'],
        # out3 = config['data']['split']['outsplit1_3']

# Count 3: reads after split1
use rule count_fq_gz_reads as count_fastgz_split1 with:
    input:
        mock = rules.first_split.output.mockout
    params:
        fqgz = config['data']['split']['fastqgz1'],
        text = config['data']['count']['fastqgz_split1']
    output:
        txt = temporary(config['data']['qc']['readnum3'])



# Step 2 splitting reads with full CapTrap linkers
use rule split_full_linker as second_split with:
    input:
        mockin2 = rules.first_split.output.mockout
    params:
        fastqdir = config['data']['split']['outdir1'],
        outdir = config['data']['split']['outdir2']
    output:
        mockout = temporary(config['data']['split']['mockout2'])        #out1 = config['data']['split']['outsplit2_1'],
        #out2 = config['data']['split']['outsplit2_2'],
        #out3 = config['data']['split']['outsplit2_3']

# Count 4: reads after split2
use rule count_fq_gz_reads as count_fastgz_split2 with:
    input:
        mock = rules.second_split.output.mockout
    params:
        fqgz = config['data']['split']['fastqgz2'],
        text = config['data']['count']['fastqgz_split2']
    output:
        txt = temporary(config['data']['qc']['readnum4'])


# Step 3 remove multisplits
use rule skip_multisplits as remove_multisplits with:
    input:
        mockinput = rules.second_split.output.mockout
    params:
        splitdir = config['data']['split']['dirroot'],
        outdir = config['data']['split']['multiout']
    output:
        mockout = temporary(config['data']['split']['mockout3']),
        fq = temporary(config['data']['split']['fastq_split'])
        #fastq_split = config['data']['split']['fastq_split']
        # fastq_multisplit = config['data']['split']['fastq_multisplit'],
        # multisplitsID1 = config['data']['split']['multisplitsID1'],
        # multisplitsID2 = config['data']['split']['multisplitsID2']

# Count 5: reads after removemultisplit
use rule count_fq_gz_reads as count_fastgz_split3 with:
    input:
        mockin = config['data']['split']['mockout3']
    params:
        fqgz = rules.remove_multisplits.output.fq,
        text = config['data']['count']['fastqgz_split3']
    output:
        txt = temporary(config['data']['qc']['readnum5'])

# get the read IDs of all reads
use rule fastqgz_get_read_ids as get_all_read_ids with:
    input:
        fq = rules.remove_multisplits.output.fq
    output:
        txt = temporary(config['data']['duplex']['read_ids'])

# START OF DEDUPLICATION
# corr. to 00_fastq2fasta.sh
use rule fq_to_fa as raw_fq_to_fa with:
    input:
        fq = config['data']['split']['fastq_split'],
        mockin= rules.remove_multisplits.output.mockout
        # fq = lambda wc: get_df_val(df, 'fname', wc.sample)
        # fq = config['raw']['fq']
    output:
        fa = temporary(config['data']['duplex']['fa'])

# Count 6: reads after fastq to fasta
use rule count_fa_reads as count_fasta_reads with:
    input:
        fa = rules.raw_fq_to_fa.output.fa
    params:
        text = temporary(config['data']['count']['fastafromfastq'])
    output:
        txt = config['data']['qc']['readnum6']

# corr. to 01_blast/00_create_db.sh
use rule make_blast_db as blast_db with:
    input:
        fa = config['ref']['linker_5_fa']
    params:
        db = config['data']['blast']['db']
    output:
        nhr = config['data']['blast']['nhr'],
        nin = config['data']['blast']['nin'],
        nsq = config['data']['blast']['nsq']

# corr to 01_blast/01_blast.sh
use rule blast as blast_seq with:
    input:
        fa = rules.raw_fq_to_fa.output.fa,
        nhr = rules.blast_db.output.nhr
    params:
        db = rules.blast_db.params.db
    output:
        xml = temporary(config['data']['blast']['xml'])

# corr to 01_blast/02_xml2sam
use rule blast2bam as blast2bam_seq with:
    input:
        ref_fa = config['ref']['linker_5_fa'],
        fa = rules.blast_seq.input.fa,
        xml = rules.blast_seq.output.xml
    params:
        blast2bam = config['bin']['blast2bam']
    output:
        sam = temporary(config['data']['blast']['sam'])

# Count 7: number of alignments to 5linker
use rule count_sam_mappings as count_total_5linker_mappings with:
    input:
        sam = rules.blast2bam_seq.output.sam
    params:
        text = config['data']['count']['5linkermappings']
    output:
        txt = temporary(config['data']['qc']['readnum7'])

# Count 8: number of primary alignments
use rule count_reads_with_primary_alignment as count_5linker_primary_alignments with:
    input:
        sam = rules.blast2bam_seq.output.sam
    params:
        text = config['data']['count']['5linkerprimarymappings']
    output:
        txt = temporary(config['data']['qc']['readnum8'])

# filter out unmapped reads
use rule sam_filt_unmapped as filt_sam_blast with:
    input:
        align = rules.blast2bam_seq.output.sam
    output:
        align = temporary(config['data']['blast']['sam_filt'])
        # align = temporaryorary(config['data']['blast']['sam_filt'])

# filter out supplementary and secondary reads (ie only primary)
use rule sam_filt_for_primary as filt_sam_blast_2 with:
    input:
        align = rules.filt_sam_blast.output.align
    output:
        align = temporary(config['data']['blast']['sam_filt_2'])

use rule sam_to_bam as sam_to_bam_blast with:
    input:
        align = rules.filt_sam_blast_2.output.align
    output:
        align = temporaryorary(config['data']['blast']['bam'])

use rule bam_sort as bam_sort_blast with:
    input:
        align = rules.sam_to_bam_blast.output.align
    output:
        align = temporary(config['data']['blast']['bam_sort'])

use rule bam_index as bam_index_blast with:
    input:
        align = rules.bam_sort_blast.output.align
    output:
        align = temporary(config['data']['blast']['bam_ind'])

# corr. to 02_extract_UMI/01_extract_UMI.sh
use rule extract_umi as extract_umi_blast with:
    input:
        align = rules.filt_sam_blast.output.align
    params:
        opref = config['data']['umi']['umis'].split('_extracted_UMI.tsv')[0],
        sep = config['params']['umi_sep']
    output:
        umis = temporary(config['data']['umi']['umis']),
        fa = temporary(config['data']['umi']['fa'])

use rule fasta_get_read_ids_rm_umi as get_umi_read_ids with:
    input:
        fa = rules.extract_umi_blast.output.fa
    output:
        txt = temporary(config['data']['umi']['read_ids'])

# Count 9: number of reads with UMI
use rule count_text_reads as count_reads_with_UMI with:
    input:
        txt = rules.get_umi_read_ids.output.txt
    params:
        text = config['data']['count']['reads_with_UMI']
    output:
        txt = temporary(config['data']['qc']['readnum9'])

# get the read ids of reads that we didn't find UMIs for
# by subtracting the reads with a UMI from the list of all reads
use rule read_id_diff as get_non_umi_read_ids with:
    input:
        a = rules.get_all_read_ids.output.txt,
        b = rules.get_umi_read_ids.output.txt
    output:
        txt = temporary(config['data']['umi']['no_umi_read_ids'])

# Count 10: number of reads without UMI
use rule count_text_reads as count_reads_without_UMI with:
    input:
        txt = rules.get_non_umi_read_ids.output.txt
    params:
        text = config['data']['count']['reads_without_UMI']
    output:
        txt = temporary(config['data']['qc']['readnum10'])

# TODO this is where the duplex read recovery will happen

rule minimap_index:
    resources:
        threads = 112
    params:
        ref = config['ref']['fa']
    output:
        index = config['ref']['mmi']
    shell:
        """
        module load minimap2

        minimap2 -t {resources.threads} -d {output.index} {params.ref}

        """


# corr. to 03_genome_mapping/01_minimap.sh
use rule minimap as map with:
    input:
        junc_bed = config['ref']['junc_bed'],
        fa = rules.extract_umi_blast.output.fa,
        ref_fa = config['ref']['mmi']
    output:
        sam = temporary(config['data']['minimap']['sam'])
        # sam = temporaryorary(config['data']['minimap']['sam'])

# rules below corr. to 03_genome_mapping/02_filter_and_transform_results
use rule align_filt_unmapped_supp as filt_sam_map with:
    input:
        align = rules.map.output.sam
    output:
        align = temporary(config['data']['minimap']['sam_filt'])

use rule sam_to_bam as sam_to_bam_map with:
    input:
        align = rules.filt_sam_map.output.align
    output:
        align = temporaryorary(config['data']['minimap']['bam'])

use rule bam_sort as bam_sort_map with:
    input:
        align = rules.sam_to_bam_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam_sort'])

use rule bam_index as bam_index_map with:
    input:
        align = rules.bam_sort_map.output.align
    output:
        align = temporary(config['data']['minimap']['bam_ind'])

# get read ID of reads with UMI but unmapped

use rule bam_get_unmapped_read_ids as get_bam_get_unmapped_read_ids with:
    input:
        bam = rules.map.output.sam
    output:
        txt = temporary(config['data']['minimap']['read_ids'])


# Count 10_2 reads with UMI but unmapped

use rule count_text_reads as count_reads_withUMI_non_deduped with:
    input:
        txt = rules.get_bam_get_unmapped_read_ids.output.txt
    params:
        text = config['data']['count']['reads_umi_non_deduped']
    output:
        txt = temporary(config['data']['qc']['readnum10_2'])

# rules below correspond to 05_deduplication/01_deduplication.sh
use rule gtf_to_gt_map as get_gt_map with:
    input:
        gtf = config['ref']['gtf']
    output:
        gt_map = config['ref']['gt_map']

use rule dedupe_umi as umi_dedupe_reads with:
    input:
        align = rules.bam_sort_map.output.align,
        mock = config['data']['minimap']['bam_ind'],
        gt_map = config['ref']['gt_map']
    params:
        sep = config['params']['umi_sep'],
        edit_dist = 2
    output:
        align = temporary(config['data']['umi_dedupe']['bam']),
        log = config['data']['umi_dedupe']['log']

# get read IDs of reads that we UMI deduplicate
use rule bam_get_read_ids as get_umi_dedupe_read_ids with:
    input:
        bam = rules.umi_dedupe_reads.output.align
    output:
        txt = temporary(config['data']['umi_dedupe']['read_ids'])

# Count 11: number of reads that could be potentially deduplicated
use rule count_text_reads as count_reads_potentially_dedup with:
    input:
        txt = rules.get_umi_dedupe_read_ids.output.txt
    params:
        text = config['data']['count']['reads_deduped']
    output:
        txt = temporary(config['data']['qc']['readnum11'])

# get bam file of
# 1. UMI deduplicated reads
# 2. reads that we didn't find UMIs for
use rule read_id_union as get_qc_reads with:
    input:
        a = rules.get_non_umi_read_ids.output.txt,
        b = rules.get_umi_dedupe_read_ids.output.txt,
        c = rules.get_bam_get_unmapped_read_ids.output.txt
    output:
        txt = temporary(config['data']['qc']['read_ids'])

# Count 11.2
use rule count_text_reads as get_reads_to_be_selected with:
    input:
        txt = rules.get_qc_reads.output.txt
    params:
        text = config['data']['count']['final_filtering']
    output:
        txt = temporary(config['data']['qc']['readnum11_2'])

use rule fastqgz_filter as make_clean_fastq with:
    input:
        fq = rules.remove_multisplits.output.fq,
        read_ids = rules.get_qc_reads.output.txt
    output:
        cleanfastq = config['data']['qc']['fq'],
        cleanfastqgz = config['data']['qc']['fqgz']

# Count 12: number of reads that could be potentially deduplicated
use rule count_fq_gz_reads as count_final_reads with:
    input:
        mockin = config['data']['qc']['fqgz']
    params:
        fqgz = rules.make_clean_fastq.output.cleanfastqgz,
        text = config['data']['count']['final_count']
    output:
        txt = temporary(config['data']['qc']['readnum12'])

# Count final: concatenate all files
rule cat_all_counts:
    resources:
        threads = 1
    input:
        a = config['data']['qc']['readnum1'],
        b = config['data']['qc']['readnum2'],
        c = config['data']['qc']['readnum3'],
        d = config['data']['qc']['readnum4'],
        e = config['data']['qc']['readnum5'],
        f = config['data']['qc']['readnum6'],
        g = config['data']['qc']['readnum7'],
        h = config['data']['qc']['readnum8'],
        i = config['data']['qc']['readnum9'],
        j = config['data']['qc']['readnum10'],
        k = config['data']['qc']['readnum10_2'],
        l = config['data']['qc']['readnum11'],
        m = config['data']['qc']['readnum11_2'],
        n = config['data']['qc']['readnum12']
    output:
        txt = config['data']['qc']['readnum']
    shell:
        """
        cat {input.a} {input.b} {input.c} {input.d} {input.e} {input.f} {input.g} {input.h} {input.i}\
        {input.j} {input.k} {input.l} {input.m} {input.n}> {output.txt}
        """

