Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, threads=8
Select jobs to execute...

[Tue May 14 11:25:15 2024]
rule remove_multisplits:
    input: data/split/mock2_sample1
    output: data/split/sample1_2step/splitReads.fastq.gz
    jobid: 0
    reason: Forced execution
    wildcards: sample=sample1
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/scratch/tmp/1738966, threads=8

/usr/bin/bash: line 2: function: No such file or directory
[Tue May 14 11:25:15 2024]
Error in rule remove_multisplits:
    jobid: 0
    input: data/split/mock2_sample1
    output: data/split/sample1_2step/splitReads.fastq.gz
    shell:
        
        bash skipMultiSplits.sh . data/split/<function <lambda> at 0x7f2339dc0a60>_postsplitting/ data/split sample1 8 data/split/mock2_sample1
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
