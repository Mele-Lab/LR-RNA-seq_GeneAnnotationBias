Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, threads=8, mem_gb=32
Select jobs to execute...

[Mon May 13 18:16:39 2024]
rule first_split:
    input: data/duplex/sample1.fq
    output: data/split/sample1_1step/sample1.fq, data/split/sample1_1step/sample1/edited.pkl, data/split/sample1_1step/sample1/split_multiple_times.pkl, data/split/sample1_1step/sample1/unedited.pkl
    jobid: 0
    reason: Forced execution
    wildcards: sample=sample1
    resources: mem_mb=1000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/scratch/tmp/1724852, threads=8, mem_gb=32

load ANACONDA/2023.07 (PATH, MANPATH, LD_LIBRARY_PATH, LIBRARY_PATH,
PKG_CONFIG_PATH, C_INCLUDE_PATH) 

CondaError: Run 'conda init' before 'conda activate'

[Mon May 13 18:16:41 2024]
Error in rule first_split:
    jobid: 0
    input: data/duplex/sample1.fq
    output: data/split/sample1_1step/sample1.fq, data/split/sample1_1step/sample1/edited.pkl, data/split/sample1_1step/sample1/split_multiple_times.pkl, data/split/sample1_1step/sample1/unedited.pkl
    shell:
        
        module load anaconda
        conda activate /gpfs/projects/bsc83/utils/conda_envs/duplextools_env
        /gpfs/projects/bsc83/utils/conda_envs/duplextools_env/bin/duplex_tools split_on_adapter --threads 8 --adapter_type ONT_sequencing_adapter+CapTrapSeqJoint data/duplex/sample1.fq data/split/sample1_1step/sample1.fq PCR
        
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
